{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SETTINGS\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "\n",
    "# LEARNING RATE SETTINGS\n",
    "BASE_LR = 0.001\n",
    "DECAY_WEIGHT = 0.1 # factor by which the learning rate is reduced.\n",
    "EPOCH_DECAY = 30 # number of epochs after which the learning rate is decayed exponentially by DECAY_WEIGHT.\n",
    "\n",
    "\n",
    "# DATASET INFO\n",
    "NUM_CLASSES = 500 # set the number of classes in your dataset\n",
    "\n",
    "\n",
    "# DATALOADER PROPERTIES\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# GPU SETTINGS\n",
    "CUDA_DEVICE = 0 # Enter device ID of your gpu if you want to run on gpu. Otherwise neglect.\n",
    "GPU_MODE = 0 # set to 1 if want to run on gpu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark: 6m45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_class import FixedUniformDataset\n",
    "\n",
    "dsets = {}\n",
    "for split in ['train', 'test']:\n",
    "    dsets[split] = FixedUniformDataset(split+'ing.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_loaders = {}\n",
    "for split in ['train', 'test']:\n",
    "    dset_loaders[split] = torch.utils.data.DataLoader(dsets[split], batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dset_loaders['train']:\n",
    "  inputs, labels = data\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500])\n",
      "tensor([201, 140, 108, 271, 325, 301,  55,  84, 449, 165, 331,  33, 298, 121,\n",
      "        419, 235, 115, 279, 417, 437, 143, 384, 476, 127, 225, 319, 246, 217,\n",
      "        240, 368, 461,  68])\n",
      "tensor([201, 140, 129,   5, 303, 156, 156,  70, 156, 165, 303,  42, 298, 121,\n",
      "        303, 117, 303, 156, 156, 156, 129, 384, 464, 156, 361, 129, 156, 214,\n",
      "        303, 156, 156,  70])\n",
      "6.099215507507324\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = Variable(inputs), Variable(labels)\n",
    "outputs = model_ft(inputs)\n",
    "print(outputs[0].shape)\n",
    "print(labels)\n",
    "_, preds = torch.max(outputs.data, 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(outputs, labels)\n",
    "print(preds)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_TENSORBOARD = False\n",
    "use_gpu = GPU_MODE\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(CUDA_DEVICE)\n",
    "\n",
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        optimizer = lr_scheduler(optimizer, epoch)\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        running_total = 0\n",
    "\n",
    "        counter=0\n",
    "        # Iterate over data, getting one batch of inputs (images) and labels each time.\n",
    "        for data in dset_loaders['train']:\n",
    "            inputs, labels = data\n",
    "\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    inputs, labels = Variable(inputs.float().cuda()), Variable(labels.long().cuda())\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR! here are the inputs and labels before we print the full stack trace:\")\n",
    "                    print(inputs, labels)\n",
    "                    raise e\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "            # outputs will be top 1 from softmax, labels will be single int\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Print a line every 10 batches so you have something to watch and don't feel like the program isn't running.\n",
    "            if counter%10==0:\n",
    "                print(\"Reached batch iteration\", counter)\n",
    "\n",
    "            counter+=1\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            try:\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_total += len(labels.data)\n",
    "            except:\n",
    "                print('unexpected error, could not calculate loss or do a sum.')\n",
    "\n",
    "            epoch_loss = running_loss / 500\n",
    "            epoch_acc = running_corrects.item() / running_total\n",
    "            # print(running_corrects, running_total)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format('training', epoch_loss, epoch_acc))\n",
    "            accuracies.append(epoch_acc)\n",
    "            losses.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('returning and looping back')\n",
    "\n",
    "    return best_model, accuracies, losses\n",
    "\n",
    "\n",
    "# This function changes the learning rate as the model trains.\n",
    "# If the learning rate is too high, training tends to be unstable and it's harder to converge on an optimal set of weights. \n",
    "# But, if learning rate is too low, learning is too slow and you won't converge in a reasonable time frame. A good compromise \n",
    "# is to start out with a high learning rate and then reduce it over time. \n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0211,  0.0113, -0.0087,  ..., -0.0041,  0.0101, -0.0163],\n",
      "        [-0.0031,  0.0187, -0.0216,  ..., -0.0219, -0.0214, -0.0043],\n",
      "        [ 0.0068,  0.0094,  0.0024,  ...,  0.0087, -0.0104,  0.0213],\n",
      "        ...,\n",
      "        [-0.0213, -0.0044, -0.0145,  ...,  0.0116,  0.0196,  0.0133],\n",
      "        [ 0.0154, -0.0214,  0.0231,  ...,  0.0066,  0.0153, -0.0227],\n",
      "        [-0.0142, -0.0119, -0.0191,  ..., -0.0030,  0.0148, -0.0126]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0025, 0.0135, 0.0140,  ..., 0.0119, 0.0168, 0.0087],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0049,  0.0241, -0.0176,  ...,  0.0102,  0.0023, -0.0180],\n",
      "        [ 0.0059, -0.0160,  0.0195,  ...,  0.0214,  0.0222,  0.0204],\n",
      "        [-0.0229,  0.0257,  0.0241,  ..., -0.0069,  0.0181, -0.0242],\n",
      "        ...,\n",
      "        [-0.0198,  0.0215,  0.0137,  ..., -0.0233, -0.0047,  0.0165],\n",
      "        [-0.0248,  0.0013, -0.0081,  ...,  0.0090, -0.0121, -0.0005],\n",
      "        [-0.0185, -0.0070,  0.0250,  ..., -0.0084, -0.0149,  0.0094]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0128,  0.0118, -0.0187,  ..., -0.0156,  0.0220, -0.0153],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0145, -0.0088, -0.0092,  ...,  0.0077, -0.0009,  0.0049],\n",
      "        [-0.0173,  0.0173, -0.0146,  ..., -0.0153, -0.0138,  0.0038],\n",
      "        [-0.0218, -0.0148,  0.0039,  ...,  0.0181, -0.0194,  0.0163],\n",
      "        ...,\n",
      "        [ 0.0171, -0.0032,  0.0108,  ...,  0.0119,  0.0177, -0.0079],\n",
      "        [-0.0087, -0.0096, -0.0231,  ...,  0.0112,  0.0151, -0.0034],\n",
      "        [-0.0013, -0.0034,  0.0117,  ..., -0.0149,  0.0168,  0.0005]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 1.7096e-02, -1.9406e-02,  2.0113e-02, -1.9748e-02,  2.1682e-02,\n",
      "        -2.9185e-03,  1.2140e-03, -5.2860e-03,  1.5951e-02,  1.2024e-02,\n",
      "        -7.6015e-03, -2.0170e-03, -1.6792e-03, -2.4985e-02, -1.2113e-02,\n",
      "        -5.2048e-03,  2.5082e-02,  2.3050e-03, -2.5069e-02, -2.1690e-02,\n",
      "        -5.7119e-03,  2.0821e-02,  2.5498e-02, -1.0478e-02, -2.1175e-02,\n",
      "        -7.7989e-03, -8.5034e-04, -2.4301e-02, -5.7340e-04, -1.1589e-02,\n",
      "        -2.1771e-02,  2.6438e-03, -1.5548e-02, -1.8600e-03, -1.7268e-02,\n",
      "         1.6787e-02,  1.1689e-02, -1.4266e-03, -2.1071e-02, -2.0040e-02,\n",
      "        -2.2741e-02, -2.1337e-02, -2.1432e-02, -6.3608e-03,  1.1684e-02,\n",
      "         2.5666e-02,  2.4277e-02,  1.5385e-02, -2.4033e-02,  1.1804e-02,\n",
      "        -1.8539e-02, -1.6078e-02, -6.5686e-03, -2.0496e-02,  1.4091e-02,\n",
      "        -2.4497e-02, -7.4012e-03,  1.2532e-02,  1.8492e-02, -6.4726e-03,\n",
      "        -9.7146e-03,  1.4799e-02, -3.2404e-04, -1.4731e-02, -1.7303e-02,\n",
      "         2.4187e-03,  1.2537e-02, -1.0585e-02,  2.1768e-02,  9.1421e-03,\n",
      "         1.7381e-02,  2.3678e-02, -2.5121e-02, -1.6177e-02, -1.4237e-02,\n",
      "         9.0693e-03,  1.0259e-03,  3.9476e-03,  1.2599e-02,  1.5611e-02,\n",
      "        -1.3502e-02,  1.8934e-02,  2.1969e-02,  3.1186e-03, -1.7217e-03,\n",
      "        -1.3143e-02, -2.1731e-02,  2.2722e-02, -9.3390e-03,  2.3787e-02,\n",
      "         1.7966e-02,  2.5179e-02,  2.2662e-02, -1.9505e-02,  1.3281e-02,\n",
      "         1.5568e-02,  1.5755e-02, -2.0584e-02,  1.5895e-02, -3.5676e-03,\n",
      "         2.1371e-02,  6.2887e-03,  2.0559e-02,  1.8974e-02, -3.2538e-03,\n",
      "         2.0095e-02, -2.2908e-02, -5.2859e-03, -7.9969e-03, -6.7091e-04,\n",
      "         1.1745e-02, -2.2659e-02, -1.9541e-02,  2.0738e-02, -2.2584e-02,\n",
      "        -2.0742e-02,  2.1119e-02, -1.6227e-02,  8.5613e-03,  4.1031e-03,\n",
      "        -1.2169e-02, -1.4427e-02,  8.3213e-03, -1.4122e-03,  8.1499e-03,\n",
      "        -2.3066e-02,  1.0230e-02, -8.7416e-03, -7.5082e-03,  1.5679e-02,\n",
      "         2.7701e-03,  1.7969e-02, -2.4186e-02,  9.0534e-03,  2.6907e-03,\n",
      "         1.0541e-02,  2.1520e-02, -3.0590e-03,  2.3407e-02, -2.3596e-02,\n",
      "        -1.2858e-02, -8.4995e-04,  1.2335e-02, -1.4933e-02, -1.7641e-02,\n",
      "        -4.8185e-03, -4.9484e-03, -3.3282e-03,  7.4227e-03, -1.3417e-02,\n",
      "         2.4674e-02, -1.6715e-02,  8.6413e-03,  2.0028e-03,  7.5100e-05,\n",
      "        -1.4913e-02, -7.0943e-03,  2.0210e-02,  2.7213e-05,  8.3570e-03,\n",
      "         1.3527e-02, -1.8753e-02, -1.9044e-02,  1.9794e-02, -1.7553e-02,\n",
      "        -2.0880e-02, -1.0386e-02, -1.5098e-02, -9.8060e-04, -2.5661e-02,\n",
      "         1.7485e-02, -1.1925e-02,  3.5138e-04, -5.4243e-03,  1.5961e-02,\n",
      "         1.6727e-02, -2.2748e-02, -9.9384e-03,  9.2855e-03, -1.6710e-02,\n",
      "        -6.6718e-03, -8.8846e-03, -2.3342e-04, -2.5516e-02,  3.4247e-03,\n",
      "        -1.3456e-02,  2.1122e-02, -2.0587e-02, -9.3871e-03, -1.4856e-02,\n",
      "         2.0215e-02, -1.4780e-02,  2.0334e-02,  3.7965e-03, -3.9886e-03,\n",
      "        -1.5074e-02, -1.9776e-02, -6.1938e-03,  1.7043e-02,  2.1902e-02,\n",
      "         6.1593e-03, -6.9828e-03,  2.5360e-02,  1.8741e-02,  2.3911e-02,\n",
      "        -1.9260e-02,  1.6288e-02,  1.0373e-02, -1.7155e-02, -1.0488e-03,\n",
      "         9.1074e-03,  1.7447e-02, -1.8609e-02, -1.7734e-02, -2.0901e-02,\n",
      "         3.8667e-03, -2.4400e-02, -1.9239e-02, -6.2181e-04, -1.6363e-02,\n",
      "         2.1113e-02, -2.2572e-02, -1.2355e-02,  2.5054e-03, -4.6237e-03,\n",
      "         2.1124e-02, -6.6371e-03,  2.4659e-02,  1.5108e-03, -7.6903e-04,\n",
      "         2.2650e-02,  1.6199e-02, -6.7435e-03, -5.7685e-04, -6.9324e-03,\n",
      "         1.4562e-02, -1.6981e-02,  2.0023e-02,  1.9276e-02, -2.1253e-02,\n",
      "        -1.8501e-02,  1.8576e-02,  2.4364e-02,  7.2366e-03,  1.9700e-02,\n",
      "        -2.5113e-03, -9.8480e-03,  1.6771e-02,  3.1501e-03, -1.6332e-02,\n",
      "        -1.0619e-03,  2.4664e-02, -2.4016e-02,  2.3644e-03,  1.8994e-02,\n",
      "        -2.3701e-02, -1.0883e-02,  2.3499e-02,  1.3696e-02, -1.3034e-02,\n",
      "         2.2945e-03,  1.8223e-02, -2.5051e-02, -2.2305e-02,  1.1004e-02,\n",
      "         1.4124e-02,  1.0514e-03,  1.9820e-02, -4.9472e-03, -1.0182e-03,\n",
      "         1.6827e-03,  1.0528e-04,  1.2866e-02,  1.4703e-02, -4.0491e-03,\n",
      "        -1.4256e-02,  2.3375e-02, -1.9828e-02, -2.2300e-02,  1.3048e-02,\n",
      "         3.8562e-03,  1.5255e-02, -2.4297e-02,  1.1964e-04,  1.8298e-02,\n",
      "         1.4421e-02, -1.1666e-02,  1.0901e-02,  1.4417e-02,  6.7475e-03,\n",
      "        -1.4167e-02,  1.5787e-02,  2.3468e-02,  1.6495e-02, -1.9227e-04,\n",
      "         2.0420e-02, -2.3616e-02,  1.7021e-02, -7.6762e-03, -6.8756e-03,\n",
      "         1.0845e-02, -4.9056e-03,  2.0366e-02,  2.2542e-02, -1.8228e-02,\n",
      "         2.4871e-02, -1.8303e-02,  7.5803e-03,  2.5320e-02,  1.6934e-02,\n",
      "         2.4424e-02,  4.2292e-03,  3.1696e-03,  2.4913e-02, -8.6415e-03,\n",
      "         6.3092e-03,  5.7207e-03, -2.2889e-02, -1.6327e-03,  9.0941e-03,\n",
      "        -2.8137e-03,  1.7287e-02, -1.9433e-02, -1.1469e-02, -5.0703e-04,\n",
      "         1.1717e-02, -2.2549e-02, -6.1426e-04,  2.2894e-02, -1.9723e-02,\n",
      "         1.4481e-02, -2.4854e-02,  2.2094e-02, -3.1723e-03,  4.3635e-03,\n",
      "        -2.4151e-02,  1.8322e-02,  1.4349e-02,  1.3405e-02,  2.0440e-02,\n",
      "        -2.3663e-02,  8.0325e-03, -9.0672e-03, -1.1505e-02, -8.6926e-03,\n",
      "         2.3665e-04, -1.9365e-02, -6.1638e-03, -1.8112e-02,  1.1433e-02,\n",
      "        -5.9781e-03,  1.1354e-02, -1.2116e-03,  1.6294e-02, -1.7693e-02,\n",
      "         1.5337e-02,  2.1298e-02,  2.3208e-02,  2.5398e-02, -4.9652e-03,\n",
      "        -2.7633e-03,  1.5373e-02,  7.2015e-03,  1.5722e-02, -1.6467e-02,\n",
      "        -1.3675e-02, -1.0711e-02,  6.4453e-03, -2.0557e-02, -5.8094e-04,\n",
      "         1.1726e-02,  8.8591e-03, -1.1019e-02, -2.0596e-02, -2.1804e-02,\n",
      "         5.3212e-03, -1.2385e-02, -1.0651e-02, -1.2953e-02,  8.8621e-03,\n",
      "         9.1049e-03, -3.8726e-03,  1.4660e-02,  1.8710e-02,  1.6200e-02,\n",
      "         2.5477e-02,  5.3983e-03,  3.0662e-03, -3.8131e-03,  2.3180e-02,\n",
      "        -3.0028e-03, -1.4490e-02,  2.5828e-03,  2.4176e-02,  5.6019e-03,\n",
      "        -6.1256e-03, -1.5332e-02,  2.0987e-02, -1.7790e-02, -1.3803e-02,\n",
      "        -6.0103e-03, -2.5172e-02, -2.4223e-02, -8.6397e-04,  4.7845e-03,\n",
      "         5.8847e-03,  1.7990e-02,  1.6706e-02,  4.9758e-03,  7.1654e-03,\n",
      "         7.3626e-04, -2.3941e-02, -4.1638e-03,  1.0482e-02, -7.8562e-03,\n",
      "         2.5171e-02,  2.3754e-02,  3.9794e-04,  2.0642e-02,  2.5531e-03,\n",
      "        -1.6902e-02,  2.2600e-02,  9.4406e-03,  2.4064e-02, -2.2119e-02,\n",
      "         1.3092e-02,  1.1359e-02, -3.7218e-03, -9.2062e-03, -2.5533e-02,\n",
      "         3.2433e-03,  1.0236e-02,  7.6496e-03, -8.7497e-03, -2.5296e-02,\n",
      "        -1.7076e-02, -1.0710e-02,  6.6708e-03, -9.8319e-03,  2.5214e-03,\n",
      "        -1.4816e-02,  5.3228e-03, -1.6026e-03, -2.2052e-02, -1.5182e-03,\n",
      "         6.2019e-03,  1.9906e-02,  2.1975e-02, -2.2963e-02, -1.7632e-02,\n",
      "        -2.2743e-02, -1.8795e-02, -2.2153e-02, -9.4644e-03,  9.9434e-03,\n",
      "         1.2079e-02,  2.4095e-02,  1.1228e-03, -7.5743e-03, -2.3884e-02,\n",
      "         3.2971e-03,  5.1668e-03,  2.3138e-02, -2.4251e-02, -2.3074e-02,\n",
      "        -9.9140e-03, -1.6417e-02,  1.2919e-02,  3.2007e-03, -1.3456e-03,\n",
      "        -8.5309e-03, -1.6332e-02, -1.4255e-02, -2.3133e-02,  1.5384e-02,\n",
      "         8.8579e-03,  2.1580e-02, -6.5241e-03,  2.5129e-02, -1.0184e-02,\n",
      "         3.5647e-03,  2.3592e-03, -1.3462e-02,  2.5764e-02,  7.2846e-03,\n",
      "         1.2881e-02,  2.1698e-02,  1.2625e-02, -2.4885e-02, -1.9761e-03,\n",
      "        -1.8180e-02, -2.0020e-03,  1.5180e-02,  1.6430e-02,  1.1904e-03,\n",
      "         7.4612e-03,  4.6013e-03, -1.8009e-03, -1.8654e-02, -2.0411e-02],\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class ThreeLayerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ThreeLayerModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "model_ft = ThreeLayerModel(1781, 1500, NUM_CLASSES)\n",
    "print(list(model_ft.parameters()))\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# if use_gpu:\n",
    "#     criterion.cuda()\n",
    "#     model_ft.cuda()\n",
    "\n",
    "optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7a2acb9e50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jocelyn/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "  File \"/Users/jocelyn/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1436, in _shutdown_workers\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_ft = ThreeLayerModel(1781, 1520, NUM_CLASSES)\n",
    "model_ft.load_state_dict(torch.load('small_unif.pt'))\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/14\n",
      "----------\n",
      "LR is set to 0.001\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 1/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 2/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 3/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 4/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 5/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 6/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 7/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 8/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 9/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 10/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 11/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 12/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 13/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "----------\n",
      "Epoch 14/14\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "training Loss: 0.0124 Acc: 0.0000\n",
      "training Loss: 0.0249 Acc: 0.0000\n",
      "training Loss: 0.0373 Acc: 0.0000\n",
      "training Loss: 0.0497 Acc: 0.0000\n",
      "training Loss: 0.0621 Acc: 0.0000\n",
      "training Loss: 0.0746 Acc: 0.0000\n",
      "training Loss: 0.0870 Acc: 0.0000\n",
      "training Loss: 0.0994 Acc: 0.0000\n",
      "training Loss: 0.1119 Acc: 0.0000\n",
      "training Loss: 0.1243 Acc: 0.0000\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1367 Acc: 0.0000\n",
      "training Loss: 0.1492 Acc: 0.0000\n",
      "training Loss: 0.1616 Acc: 0.0000\n",
      "training Loss: 0.1740 Acc: 0.0000\n",
      "training Loss: 0.1864 Acc: 0.0000\n",
      "training Loss: 0.1989 Acc: 0.0000\n",
      "Training complete in 15m 4s\n",
      "Best val Acc: 0.000000\n",
      "returning and looping back\n",
      "train accuracies by epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "train losses by epoch: [0.01242923355102539, 0.024858468055725096, 0.0372876672744751, 0.049716894149780276, 0.062146098136901855, 0.0745753231048584, 0.08700452613830566, 0.0994337329864502, 0.11186295986175537, 0.12429214668273926, 0.13672137260437012, 0.14915058040618898, 0.16157977485656738, 0.1740089979171753, 0.18643823051452638, 0.19886748695373535, 0.012429194450378418, 0.02485843849182129, 0.03728765106201172, 0.04971688175201416, 0.062146115303039554, 0.07457531070709228, 0.08700454711914063, 0.09943377494812011, 0.11186297512054444, 0.12429220294952392, 0.13672139739990236, 0.1491505823135376, 0.16157980442047118, 0.17400903511047364, 0.18643824577331544, 0.19886747741699218, 0.012429202079772949, 0.02485843563079834, 0.0372876501083374, 0.049716872215271, 0.062146098136901855, 0.07457530689239501, 0.08700453472137451, 0.09943373394012452, 0.11186297702789307, 0.12429218292236328, 0.13672141742706298, 0.14915065002441405, 0.16157983112335206, 0.17400903034210205, 0.18643823623657227, 0.19886748695373535, 0.012429247856140137, 0.02485844612121582, 0.03728766918182373, 0.04971688652038574, 0.062146086692810056, 0.07457529735565185, 0.08700451564788818, 0.09943374824523926, 0.1118629608154297, 0.12429218482971191, 0.13672138690948488, 0.14915060806274413, 0.1615798225402832, 0.17400903797149658, 0.1864382438659668, 0.19886748027801512, 0.01242918872833252, 0.024858429908752443, 0.03728763961791992, 0.04971685218811035, 0.062146064758300784, 0.07457531356811524, 0.08700454521179199, 0.09943377590179443, 0.11186301898956298, 0.12429224014282227, 0.13672141551971437, 0.14915060710906983, 0.1615798282623291, 0.1740090217590332, 0.18643826580047607, 0.19886746978759764, 0.012429195404052734, 0.024858417510986327, 0.03728762817382812, 0.049716858863830565, 0.062146082878112796, 0.07457531356811524, 0.08700453567504883, 0.09943374919891357, 0.11186295986175537, 0.12429219245910644, 0.13672143268585205, 0.1491506462097168, 0.16157983779907226, 0.1740090503692627, 0.1864382734298706, 0.19886746883392334, 0.012429250717163085, 0.02485844898223877, 0.03728766345977783, 0.0497168664932251, 0.062146069526672364, 0.0745752592086792, 0.08700450229644775, 0.09943371200561524, 0.11186293220520019, 0.12429214191436767, 0.13672138690948488, 0.1491506175994873, 0.16157982730865478, 0.17400903987884522, 0.18643825244903564, 0.19886748123168946, 0.012429222106933594, 0.024858429908752443, 0.03728763866424561, 0.0497168550491333, 0.06214607429504394, 0.07457530212402344, 0.0870044813156128, 0.09943369483947753, 0.11186290836334228, 0.12429209518432617, 0.13672132968902587, 0.14915055656433104, 0.1615797882080078, 0.1740090055465698, 0.18643825435638428, 0.1988674783706665, 0.01242922306060791, 0.02485841941833496, 0.03728763389587402, 0.04971683692932129, 0.0621460599899292, 0.0745752763748169, 0.08700450134277343, 0.09943373584747314, 0.11186292839050294, 0.1242921257019043, 0.1367213487625122, 0.1491505937576294, 0.1615798282623291, 0.17400904750823976, 0.1864382677078247, 0.19886746788024903, 0.012429231643676758, 0.024858442306518555, 0.03728764057159424, 0.04971686172485352, 0.06214608573913574, 0.07457529067993164, 0.0870044937133789, 0.09943371105194092, 0.1118629264831543, 0.12429210472106933, 0.13672133350372315, 0.14915056133270263, 0.16157978534698486, 0.17400904369354248, 0.18643824195861816, 0.1988674831390381, 0.012429202079772949, 0.02485840320587158, 0.037287591934204105, 0.04971682071685791, 0.06214603996276855, 0.07457525157928467, 0.08700447559356689, 0.09943369674682617, 0.11186291694641114, 0.1242921495437622, 0.13672134590148927, 0.14915055179595949, 0.161579758644104, 0.17400901985168457, 0.18643826007843017, 0.19886747741699218, 0.012429253578186035, 0.024858479499816896, 0.0372876558303833, 0.04971687316894531, 0.062146092414855955, 0.07457531070709228, 0.08700452136993408, 0.09943371677398681, 0.11186290550231934, 0.12429213047027587, 0.13672137069702148, 0.14915058135986328, 0.1615798168182373, 0.17400904273986817, 0.18643825244903564, 0.19886748218536376, 0.012429201126098632, 0.024858407974243164, 0.03728763961791992, 0.049716848373413086, 0.06214610481262207, 0.07457530879974365, 0.08700452136993408, 0.0994337739944458, 0.11186298561096192, 0.12429215717315674, 0.1367213659286499, 0.14915057468414306, 0.16157980442047118, 0.1740090322494507, 0.18643826484680176, 0.1988674850463867, 0.012429208755493163, 0.02485840606689453, 0.037287625312805175, 0.04971685028076172, 0.06214604473114014, 0.07457528209686279, 0.08700448703765869, 0.09943371295928954, 0.11186292552947998, 0.12429216575622559, 0.1367213716506958, 0.1491505880355835, 0.16157982730865478, 0.17400900650024415, 0.18643824577331544, 0.19886747741699218, 0.012429220199584961, 0.02485841464996338, 0.03728764820098877, 0.0497168607711792, 0.06214607238769531, 0.07457528305053711, 0.08700446701049805, 0.09943367767333984, 0.11186294174194336, 0.12429216003417969, 0.13672136402130128, 0.1491505823135376, 0.16157982063293458, 0.1740090503692627, 0.18643825721740723, 0.19886747741699218]\n",
      "val accuracies by epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "val losses by epoch: [0.01242923355102539, 0.024858468055725096, 0.0372876672744751, 0.049716894149780276, 0.062146098136901855, 0.0745753231048584, 0.08700452613830566, 0.0994337329864502, 0.11186295986175537, 0.12429214668273926, 0.13672137260437012, 0.14915058040618898, 0.16157977485656738, 0.1740089979171753, 0.18643823051452638, 0.19886748695373535, 0.012429194450378418, 0.02485843849182129, 0.03728765106201172, 0.04971688175201416, 0.062146115303039554, 0.07457531070709228, 0.08700454711914063, 0.09943377494812011, 0.11186297512054444, 0.12429220294952392, 0.13672139739990236, 0.1491505823135376, 0.16157980442047118, 0.17400903511047364, 0.18643824577331544, 0.19886747741699218, 0.012429202079772949, 0.02485843563079834, 0.0372876501083374, 0.049716872215271, 0.062146098136901855, 0.07457530689239501, 0.08700453472137451, 0.09943373394012452, 0.11186297702789307, 0.12429218292236328, 0.13672141742706298, 0.14915065002441405, 0.16157983112335206, 0.17400903034210205, 0.18643823623657227, 0.19886748695373535, 0.012429247856140137, 0.02485844612121582, 0.03728766918182373, 0.04971688652038574, 0.062146086692810056, 0.07457529735565185, 0.08700451564788818, 0.09943374824523926, 0.1118629608154297, 0.12429218482971191, 0.13672138690948488, 0.14915060806274413, 0.1615798225402832, 0.17400903797149658, 0.1864382438659668, 0.19886748027801512, 0.01242918872833252, 0.024858429908752443, 0.03728763961791992, 0.04971685218811035, 0.062146064758300784, 0.07457531356811524, 0.08700454521179199, 0.09943377590179443, 0.11186301898956298, 0.12429224014282227, 0.13672141551971437, 0.14915060710906983, 0.1615798282623291, 0.1740090217590332, 0.18643826580047607, 0.19886746978759764, 0.012429195404052734, 0.024858417510986327, 0.03728762817382812, 0.049716858863830565, 0.062146082878112796, 0.07457531356811524, 0.08700453567504883, 0.09943374919891357, 0.11186295986175537, 0.12429219245910644, 0.13672143268585205, 0.1491506462097168, 0.16157983779907226, 0.1740090503692627, 0.1864382734298706, 0.19886746883392334, 0.012429250717163085, 0.02485844898223877, 0.03728766345977783, 0.0497168664932251, 0.062146069526672364, 0.0745752592086792, 0.08700450229644775, 0.09943371200561524, 0.11186293220520019, 0.12429214191436767, 0.13672138690948488, 0.1491506175994873, 0.16157982730865478, 0.17400903987884522, 0.18643825244903564, 0.19886748123168946, 0.012429222106933594, 0.024858429908752443, 0.03728763866424561, 0.0497168550491333, 0.06214607429504394, 0.07457530212402344, 0.0870044813156128, 0.09943369483947753, 0.11186290836334228, 0.12429209518432617, 0.13672132968902587, 0.14915055656433104, 0.1615797882080078, 0.1740090055465698, 0.18643825435638428, 0.1988674783706665, 0.01242922306060791, 0.02485841941833496, 0.03728763389587402, 0.04971683692932129, 0.0621460599899292, 0.0745752763748169, 0.08700450134277343, 0.09943373584747314, 0.11186292839050294, 0.1242921257019043, 0.1367213487625122, 0.1491505937576294, 0.1615798282623291, 0.17400904750823976, 0.1864382677078247, 0.19886746788024903, 0.012429231643676758, 0.024858442306518555, 0.03728764057159424, 0.04971686172485352, 0.06214608573913574, 0.07457529067993164, 0.0870044937133789, 0.09943371105194092, 0.1118629264831543, 0.12429210472106933, 0.13672133350372315, 0.14915056133270263, 0.16157978534698486, 0.17400904369354248, 0.18643824195861816, 0.1988674831390381, 0.012429202079772949, 0.02485840320587158, 0.037287591934204105, 0.04971682071685791, 0.06214603996276855, 0.07457525157928467, 0.08700447559356689, 0.09943369674682617, 0.11186291694641114, 0.1242921495437622, 0.13672134590148927, 0.14915055179595949, 0.161579758644104, 0.17400901985168457, 0.18643826007843017, 0.19886747741699218, 0.012429253578186035, 0.024858479499816896, 0.0372876558303833, 0.04971687316894531, 0.062146092414855955, 0.07457531070709228, 0.08700452136993408, 0.09943371677398681, 0.11186290550231934, 0.12429213047027587, 0.13672137069702148, 0.14915058135986328, 0.1615798168182373, 0.17400904273986817, 0.18643825244903564, 0.19886748218536376, 0.012429201126098632, 0.024858407974243164, 0.03728763961791992, 0.049716848373413086, 0.06214610481262207, 0.07457530879974365, 0.08700452136993408, 0.0994337739944458, 0.11186298561096192, 0.12429215717315674, 0.1367213659286499, 0.14915057468414306, 0.16157980442047118, 0.1740090322494507, 0.18643826484680176, 0.1988674850463867, 0.012429208755493163, 0.02485840606689453, 0.037287625312805175, 0.04971685028076172, 0.06214604473114014, 0.07457528209686279, 0.08700448703765869, 0.09943371295928954, 0.11186292552947998, 0.12429216575622559, 0.1367213716506958, 0.1491505880355835, 0.16157982730865478, 0.17400900650024415, 0.18643824577331544, 0.19886747741699218, 0.012429220199584961, 0.02485841464996338, 0.03728764820098877, 0.0497168607711792, 0.06214607238769531, 0.07457528305053711, 0.08700446701049805, 0.09943367767333984, 0.11186294174194336, 0.12429216003417969, 0.13672136402130128, 0.1491505823135376, 0.16157982063293458, 0.1740090503692627, 0.18643825721740723, 0.19886747741699218]\n"
     ]
    }
   ],
   "source": [
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft, accuracies, losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    print(split, \"accuracies by epoch:\", accuracies)\n",
    "    print(split, \"losses by epoch:\", losses)\n",
    "\n",
    "# Save model\n",
    "torch.save(model_ft.state_dict(), 'med_unif.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small model maxes out at 0.668 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
