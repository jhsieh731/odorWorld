{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SETTINGS\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "\n",
    "# LEARNING RATE SETTINGS\n",
    "BASE_LR = 0.001\n",
    "DECAY_WEIGHT = 0.1 # factor by which the learning rate is reduced.\n",
    "EPOCH_DECAY = 30 # number of epochs after which the learning rate is decayed exponentially by DECAY_WEIGHT.\n",
    "\n",
    "\n",
    "# DATASET INFO\n",
    "NUM_CLASSES = 500 # set the number of classes in your dataset\n",
    "\n",
    "\n",
    "# DATALOADER PROPERTIES\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# GPU SETTINGS\n",
    "CUDA_DEVICE = 0 # Enter device ID of your gpu if you want to run on gpu. Otherwise neglect.\n",
    "GPU_MODE = 0 # set to 1 if want to run on gpu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark: 6m45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_class import FixedUniformDataset\n",
    "\n",
    "dsets = {}\n",
    "# for split in ['train', 'test']:\n",
    "for split in ['train']:\n",
    "    dsets[split] = FixedUniformDataset(split+'ing.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_loaders = {}\n",
    "# for split in ['train', 'test']:\n",
    "for split in ['train']:\n",
    "    dset_loaders[split] = torch.utils.data.DataLoader(dsets[split], batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "for data in dset_loaders['train']:\n",
    "  inputs, labels = data\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500])\n",
      "tensor([346, 102, 357, 345, 344, 355,  72, 192,  11, 412, 448, 477, 261, 495,\n",
      "        452, 108, 485, 246, 266, 118, 449, 293, 488, 104, 174, 309,  88, 212,\n",
      "         68, 362,  39,  79])\n",
      "tensor([346, 102, 357, 345, 344, 359,  74, 192,  17, 424, 359, 494, 431, 442,\n",
      "        359, 108, 485, 246, 266, 118, 359, 293, 489, 104, 174, 309,  88, 359,\n",
      "         74, 362,  42,  74])\n",
      "5.6702728271484375\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = Variable(inputs), Variable(labels)\n",
    "outputs = model_ft(inputs)\n",
    "print(outputs[0].shape)\n",
    "print(labels)\n",
    "_, preds = torch.max(outputs.data, 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(outputs, labels)\n",
    "print(preds)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_TENSORBOARD = False\n",
    "use_gpu = GPU_MODE\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(CUDA_DEVICE)\n",
    "\n",
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        # optimizer = lr_scheduler(optimizer, epoch)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        running_total = 0\n",
    "\n",
    "        counter=0\n",
    "        # Iterate over data, getting one batch of inputs (images) and labels each time.\n",
    "        for data in dset_loaders['train']:\n",
    "            inputs, labels = data\n",
    "\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    inputs, labels = Variable(inputs.float().cuda()), Variable(labels.long().cuda())\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR! here are the inputs and labels before we print the full stack trace:\")\n",
    "                    print(inputs, labels)\n",
    "                    raise e\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "            # outputs will be top 1 from softmax, labels will be single int\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Print a line every 10 batches so you have something to watch and don't feel like the program isn't running.\n",
    "            if counter%10==0:\n",
    "                print(\"Reached batch iteration\", counter)\n",
    "\n",
    "            counter+=1\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            try:\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_total += len(labels.data)\n",
    "            except:\n",
    "                print('unexpected error, could not calculate loss or do a sum.')\n",
    "\n",
    "            epoch_loss = running_loss / 500\n",
    "            epoch_acc = running_corrects.item() / running_total\n",
    "            # print(running_corrects, running_total)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format('training', epoch_loss, epoch_acc))\n",
    "            accuracies.append(epoch_acc)\n",
    "            losses.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('returning and looping back')\n",
    "\n",
    "    return best_model, accuracies, losses\n",
    "\n",
    "\n",
    "# This function changes the learning rate as the model trains.\n",
    "# If the learning rate is too high, training tends to be unstable and it's harder to converge on an optimal set of weights. \n",
    "# But, if learning rate is too low, learning is too slow and you won't converge in a reasonable time frame. A good compromise \n",
    "# is to start out with a high learning rate and then reduce it over time. \n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.0600e-02, -4.1639e-03, -1.7228e-02,  ..., -1.7263e-02,\n",
      "         -8.3633e-06, -1.8699e-02],\n",
      "        [-1.5243e-02,  1.4260e-02,  2.3253e-02,  ..., -9.9739e-03,\n",
      "         -2.3216e-02, -1.8259e-02],\n",
      "        [-1.3728e-02,  1.3375e-02, -2.1706e-03,  ...,  1.8029e-02,\n",
      "          1.0457e-02, -2.1160e-02],\n",
      "        ...,\n",
      "        [ 1.0232e-02,  2.1694e-02, -1.2204e-03,  ...,  1.5188e-02,\n",
      "          9.7068e-03, -8.0177e-03],\n",
      "        [ 2.2638e-02, -1.1897e-02, -1.6493e-02,  ...,  1.0579e-02,\n",
      "          1.0479e-03, -1.2000e-02],\n",
      "        [ 2.4121e-03, -8.3900e-03, -1.8227e-02,  ...,  1.2980e-02,\n",
      "         -3.3715e-03,  3.1476e-03]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0236,  0.0053, -0.0048,  ...,  0.0145, -0.0056,  0.0196],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0168, -0.0145,  0.0215,  ...,  0.0191, -0.0153, -0.0139],\n",
      "        [-0.0184,  0.0044, -0.0039,  ..., -0.0156, -0.0018, -0.0247],\n",
      "        [-0.0177,  0.0087, -0.0042,  ..., -0.0119, -0.0213,  0.0009],\n",
      "        ...,\n",
      "        [ 0.0065, -0.0202, -0.0066,  ...,  0.0231, -0.0148, -0.0176],\n",
      "        [ 0.0038,  0.0024, -0.0105,  ...,  0.0037, -0.0031,  0.0009],\n",
      "        [ 0.0029,  0.0227,  0.0241,  ..., -0.0114,  0.0255, -0.0173]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0187,  0.0054, -0.0019,  ..., -0.0025,  0.0210, -0.0057],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0229, -0.0092,  0.0155,  ...,  0.0154, -0.0023,  0.0167],\n",
      "        [ 0.0197, -0.0186,  0.0017,  ...,  0.0176,  0.0249,  0.0050],\n",
      "        [ 0.0190, -0.0011,  0.0122,  ..., -0.0052, -0.0257, -0.0210],\n",
      "        ...,\n",
      "        [-0.0023, -0.0141, -0.0257,  ..., -0.0187,  0.0012, -0.0150],\n",
      "        [ 0.0215,  0.0064, -0.0163,  ..., -0.0005,  0.0024, -0.0067],\n",
      "        [ 0.0237, -0.0189,  0.0103,  ..., -0.0191,  0.0101, -0.0222]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-2.4674e-02, -1.7283e-02,  1.2988e-02, -2.2677e-02, -1.5057e-02,\n",
      "         1.6205e-02,  1.2682e-02,  1.8550e-02, -2.4847e-02,  1.3166e-02,\n",
      "         1.7774e-02, -1.7057e-02,  1.9222e-02, -1.1606e-02, -1.4730e-02,\n",
      "        -1.3073e-02, -1.9102e-02, -1.7857e-02,  7.5389e-03,  1.9552e-02,\n",
      "         2.2887e-02,  1.6747e-02, -1.6894e-03, -1.5435e-02, -5.8772e-03,\n",
      "         3.4807e-03,  5.0409e-04, -5.3505e-04, -1.7510e-02, -5.2153e-03,\n",
      "        -2.2302e-02,  1.6574e-02,  5.1792e-03, -6.9723e-04, -1.1182e-02,\n",
      "        -8.2241e-03, -1.5110e-02, -6.5881e-03, -1.7880e-02, -2.5530e-02,\n",
      "         1.9468e-02, -5.0130e-03,  3.3235e-03, -8.9135e-03,  1.6575e-02,\n",
      "         2.2915e-02, -2.4407e-02,  2.4683e-02, -2.2341e-02, -1.7011e-02,\n",
      "         2.0215e-02,  1.9577e-02, -1.8688e-02, -7.1113e-03,  2.7643e-03,\n",
      "         2.5509e-02, -1.9563e-02,  8.6017e-03,  2.3369e-02,  1.0419e-02,\n",
      "        -1.6989e-04, -9.4187e-03,  2.5337e-02,  2.0936e-02,  1.1300e-03,\n",
      "         1.3778e-02,  2.3546e-02, -1.1286e-02, -1.3106e-02, -1.5864e-02,\n",
      "        -8.2114e-03, -1.2250e-02, -1.3387e-02, -2.3569e-02, -1.9304e-02,\n",
      "         1.6853e-02, -1.8112e-02,  2.5573e-02,  2.5690e-02, -1.3672e-02,\n",
      "        -1.4677e-02, -1.2621e-02, -1.9229e-02, -2.4704e-02,  1.2416e-02,\n",
      "         2.4037e-02,  2.2554e-02,  7.2163e-03,  9.8957e-03, -3.0322e-05,\n",
      "        -2.5368e-02, -1.7743e-02, -2.3881e-02, -1.3497e-02, -2.1914e-03,\n",
      "         1.6345e-02, -2.4189e-02,  2.4348e-02,  4.6322e-03,  3.0436e-03,\n",
      "        -2.3812e-02, -6.6365e-03, -6.5750e-03,  1.7011e-02, -2.5599e-03,\n",
      "         9.1252e-03, -1.9852e-02, -1.1475e-03, -1.3008e-02, -1.3579e-02,\n",
      "        -7.3570e-03,  8.9329e-03, -2.2429e-02,  4.9451e-03,  1.1045e-02,\n",
      "         8.9817e-03,  2.2467e-02, -1.8579e-02, -6.4796e-03, -2.3588e-02,\n",
      "        -8.1668e-03, -1.2125e-04, -1.4150e-02,  2.2749e-02, -1.3507e-03,\n",
      "         2.1878e-02,  1.3301e-02, -1.7164e-02, -1.2720e-02, -5.4088e-03,\n",
      "        -2.3361e-02, -2.5379e-02,  2.0427e-02,  8.3535e-03,  1.1636e-02,\n",
      "         3.0009e-03, -1.9000e-02, -2.2360e-02, -8.0890e-04, -6.7673e-03,\n",
      "         2.0916e-02, -1.5016e-02, -1.8216e-02,  2.4619e-02,  2.1729e-02,\n",
      "         9.6517e-03, -2.0063e-02,  1.3621e-03, -4.8048e-03, -1.4497e-02,\n",
      "        -1.3909e-02, -2.5788e-02, -2.4680e-02, -2.0068e-02, -1.5989e-02,\n",
      "        -1.7213e-02,  2.3599e-02, -2.4749e-02, -3.7336e-03, -1.3285e-02,\n",
      "        -2.0980e-02, -4.4680e-03,  1.1092e-03, -4.8142e-03, -2.5050e-03,\n",
      "         1.8427e-02, -1.6621e-02, -2.4658e-03,  1.1516e-02, -3.0973e-03,\n",
      "        -2.4658e-02, -4.0372e-03, -8.1626e-03, -1.9853e-02,  8.2400e-03,\n",
      "         1.5549e-02,  3.6379e-03,  1.1521e-02, -1.2103e-02,  4.3834e-03,\n",
      "        -5.6852e-03, -6.1618e-03,  1.2306e-02, -2.0895e-02,  1.2152e-02,\n",
      "        -1.8953e-02,  2.2461e-02, -1.5731e-02, -2.2441e-02, -1.3703e-05,\n",
      "        -1.5256e-02,  2.3991e-02, -5.2902e-03,  1.4428e-03,  2.0784e-03,\n",
      "        -2.0227e-02,  7.8192e-03, -2.8393e-03, -8.5733e-03,  2.2057e-03,\n",
      "         1.5221e-03,  1.7237e-02,  2.4355e-02,  5.6503e-03,  1.9341e-03,\n",
      "         2.3629e-02,  2.4871e-02,  2.4714e-02,  1.1234e-03, -2.2883e-02,\n",
      "         7.0107e-03, -2.1265e-02, -1.3166e-02,  8.9680e-03,  2.5605e-02,\n",
      "        -2.3181e-03,  1.7093e-02, -1.0292e-02,  1.4632e-02,  1.4169e-03,\n",
      "         2.0331e-02,  2.3333e-02,  2.6126e-03,  1.3396e-02, -2.3116e-02,\n",
      "         1.4220e-02,  2.4512e-02,  1.0433e-02, -1.8558e-02,  1.8994e-02,\n",
      "        -9.4824e-03,  3.5421e-03,  1.3861e-02,  1.3424e-02, -1.4640e-03,\n",
      "        -6.3058e-03, -7.0422e-03,  1.4480e-02,  2.1658e-02, -1.2856e-02,\n",
      "        -1.0580e-02, -1.0137e-02,  2.2052e-02, -1.5501e-02, -5.9566e-03,\n",
      "         1.6671e-02, -1.4701e-02, -2.5599e-02, -9.9118e-03, -7.7224e-03,\n",
      "         1.2935e-02,  4.4049e-03, -6.6750e-03, -1.6989e-02,  3.0050e-03,\n",
      "         1.6802e-02, -4.9440e-03,  2.6651e-03,  2.0138e-02, -3.5057e-03,\n",
      "        -8.6564e-03,  1.6677e-02,  1.3592e-02,  5.6461e-03, -2.0211e-02,\n",
      "         1.5582e-03,  5.4426e-03, -2.1803e-02,  1.9804e-03,  3.1547e-03,\n",
      "        -2.2208e-02,  1.1696e-02, -2.4001e-03,  3.8338e-03, -1.4038e-02,\n",
      "         7.7733e-03,  6.9029e-03,  1.9713e-02, -1.1327e-02, -1.7196e-02,\n",
      "        -1.5191e-02,  2.3923e-02,  7.3931e-03, -5.9482e-03,  2.5045e-02,\n",
      "         8.1167e-03, -1.6524e-03,  1.2547e-02,  1.0969e-02,  1.5260e-02,\n",
      "         1.8282e-02,  2.1103e-02,  1.8774e-02, -6.5259e-03, -1.9332e-02,\n",
      "         1.7385e-02,  3.9763e-03, -2.5800e-02,  1.9252e-03,  8.4502e-03,\n",
      "        -2.5525e-02,  6.6861e-03,  1.4820e-02, -2.5613e-02,  1.1752e-03,\n",
      "        -6.7347e-03,  1.2230e-02,  1.7198e-02, -2.5116e-02, -9.0708e-03,\n",
      "         1.1774e-03,  1.9350e-02,  1.9553e-02,  1.5537e-02, -1.7711e-02,\n",
      "         1.0686e-03, -1.3781e-02,  5.6226e-03, -5.8484e-03,  2.8499e-03,\n",
      "         2.2512e-02,  2.5805e-03,  1.1289e-02, -3.7772e-03,  2.3596e-02,\n",
      "         7.5599e-03, -2.2378e-02,  2.0339e-02,  1.1122e-02, -1.9002e-03,\n",
      "        -1.4988e-02, -2.0147e-02, -3.9599e-03, -1.9793e-02, -1.8121e-02,\n",
      "        -1.0491e-02,  3.5886e-03, -2.2794e-02, -4.7556e-03,  5.1176e-03,\n",
      "         2.0657e-02, -1.2796e-02, -1.2115e-02,  1.0878e-02,  1.6014e-02,\n",
      "         7.1940e-03, -8.3638e-03,  2.4700e-02, -2.0677e-02,  7.9985e-03,\n",
      "        -9.5426e-03,  1.2792e-02,  2.3214e-02,  1.3763e-02,  1.4068e-02,\n",
      "        -9.7016e-03, -2.1289e-02,  2.0167e-02, -1.9939e-02,  1.3551e-02,\n",
      "         5.9938e-04, -6.9931e-04,  9.3114e-03, -2.1132e-02,  9.2131e-03,\n",
      "        -4.6149e-03, -2.5788e-02,  2.4718e-02,  1.6710e-02, -2.5388e-03,\n",
      "         2.0453e-02,  1.0618e-02, -1.6399e-02,  1.7362e-02, -2.1160e-02,\n",
      "        -2.5787e-03, -2.3870e-02, -1.0319e-02, -5.2852e-03, -4.4019e-03,\n",
      "        -2.2296e-02,  1.1123e-02, -1.2282e-02,  1.6760e-02, -2.4266e-04,\n",
      "        -1.7335e-02, -2.1124e-02, -6.1184e-03, -1.3178e-03, -2.1270e-02,\n",
      "         2.3024e-02,  4.2725e-03,  9.6332e-03, -2.4204e-02, -1.3006e-02,\n",
      "         1.3212e-03, -2.2875e-02, -2.3478e-02,  7.8132e-03, -1.4953e-02,\n",
      "        -1.3223e-02, -4.4197e-03,  1.1319e-02,  1.7528e-02,  1.3308e-02,\n",
      "        -9.6513e-03, -1.2453e-02, -6.6322e-03, -2.4298e-02,  2.3904e-02,\n",
      "         3.3351e-03, -9.8286e-03,  1.9219e-02,  1.7504e-02, -2.3717e-02,\n",
      "        -2.5207e-02, -1.0156e-02,  1.3064e-02,  2.1270e-02,  1.7647e-02,\n",
      "        -2.3694e-03,  1.8447e-02, -2.0245e-02,  1.5982e-02,  1.5599e-02,\n",
      "        -1.6033e-02, -1.3793e-02,  2.5007e-03,  9.8203e-03, -1.3336e-02,\n",
      "         1.8380e-02,  1.4235e-03, -7.5091e-03,  1.4728e-02, -1.4888e-02,\n",
      "         1.7350e-02, -5.7147e-04,  3.7570e-03, -1.5016e-02,  1.3805e-02,\n",
      "        -2.0921e-02, -2.4613e-02, -3.3646e-03, -1.3217e-03, -3.0995e-03,\n",
      "         2.1645e-02, -2.5785e-02,  1.0706e-02,  1.8088e-02, -1.2572e-02,\n",
      "        -1.5639e-02,  2.0804e-02, -1.3342e-02, -2.3666e-02,  1.5896e-02,\n",
      "         7.6540e-03,  8.3490e-03,  7.8678e-03,  1.0928e-02,  4.2364e-03,\n",
      "        -3.9119e-03, -5.0085e-03,  2.2732e-02,  1.0062e-02,  1.6325e-02,\n",
      "         5.4486e-03,  1.3711e-03,  2.2066e-02,  1.0177e-02,  2.3374e-02,\n",
      "        -1.3446e-02, -9.3275e-03,  1.4846e-02,  5.4461e-03, -1.0059e-04,\n",
      "        -2.4234e-02,  1.3038e-02, -1.5874e-02,  6.1670e-03, -2.2621e-02,\n",
      "        -1.1270e-02,  1.4308e-02,  2.5478e-02, -2.3807e-02,  2.3973e-02,\n",
      "         1.7670e-02, -2.1104e-02, -1.2119e-02,  2.0759e-03,  4.3989e-03,\n",
      "         1.3842e-02,  3.3911e-03,  1.1898e-02, -1.7287e-02,  5.9308e-03,\n",
      "         2.2032e-02, -1.5907e-02,  1.6593e-02, -3.0725e-03, -2.4427e-03],\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class ThreeLayerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ThreeLayerModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "model_ft = ThreeLayerModel(1781, 1500, NUM_CLASSES)\n",
    "print(list(model_ft.parameters()))\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# if use_gpu:\n",
    "#     criterion.cuda()\n",
    "#     model_ft.cuda()\n",
    "\n",
    "optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThreeLayerModel(\n",
       "  (fc1): Linear(in_features=1781, out_features=1500, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=1500, out_features=1500, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=1500, out_features=500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft = ThreeLayerModel(1781, 1500, NUM_CLASSES)\n",
    "model_ft.load_state_dict(torch.load('small_unif.pt'))\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached batch iteration 0\n",
      "training Loss: 0.0111 Acc: 0.6875\n",
      "training Loss: 0.0221 Acc: 0.7031\n",
      "training Loss: 0.0330 Acc: 0.7188\n",
      "training Loss: 0.0442 Acc: 0.6953\n",
      "training Loss: 0.0552 Acc: 0.6937\n",
      "training Loss: 0.0664 Acc: 0.6823\n",
      "training Loss: 0.0775 Acc: 0.6830\n",
      "training Loss: 0.0887 Acc: 0.6758\n",
      "training Loss: 0.0999 Acc: 0.6701\n",
      "training Loss: 0.1110 Acc: 0.6687\n",
      "Reached batch iteration 10\n",
      "training Loss: 0.1220 Acc: 0.6761\n",
      "training Loss: 0.1331 Acc: 0.6719\n",
      "training Loss: 0.1441 Acc: 0.6803\n",
      "training Loss: 0.1554 Acc: 0.6719\n",
      "training Loss: 0.1664 Acc: 0.6750\n",
      "training Loss: 0.1778 Acc: 0.6680\n",
      "Training complete in 1m 0s\n",
      "Best val Acc: 0.000000\n",
      "returning and looping back\n",
      "train accuracies by epoch: [0.6875, 0.703125, 0.71875, 0.6953125, 0.69375, 0.6822916666666666, 0.6830357142857143, 0.67578125, 0.6701388888888888, 0.66875, 0.6761363636363636, 0.671875, 0.6802884615384616, 0.671875, 0.675, 0.668]\n",
      "train losses by epoch: [0.011061152458190919, 0.022061598777770995, 0.03300006103515625, 0.04418050098419189, 0.05524302577972412, 0.06642905902862549, 0.07750120830535889, 0.08870628070831299, 0.09989509391784668, 0.11101795578002929, 0.12195591259002686, 0.1331426877975464, 0.14407965755462646, 0.15538473320007323, 0.1664350299835205, 0.1778391227722168]\n",
      "val accuracies by epoch: [0.6875, 0.703125, 0.71875, 0.6953125, 0.69375, 0.6822916666666666, 0.6830357142857143, 0.67578125, 0.6701388888888888, 0.66875, 0.6761363636363636, 0.671875, 0.6802884615384616, 0.671875, 0.675, 0.668]\n",
      "val losses by epoch: [0.011061152458190919, 0.022061598777770995, 0.03300006103515625, 0.04418050098419189, 0.05524302577972412, 0.06642905902862549, 0.07750120830535889, 0.08870628070831299, 0.09989509391784668, 0.11101795578002929, 0.12195591259002686, 0.1331426877975464, 0.14407965755462646, 0.15538473320007323, 0.1664350299835205, 0.1778391227722168]\n"
     ]
    }
   ],
   "source": [
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft, accuracies, losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=1)\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    print(split, \"accuracies by epoch:\", accuracies)\n",
    "    print(split, \"losses by epoch:\", losses)\n",
    "\n",
    "# Save model\n",
    "# torch.save(model_ft.state_dict(), 'med_unif.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small model maxes out at 0.668 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
